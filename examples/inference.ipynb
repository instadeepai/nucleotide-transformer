{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a201d7bf-d4d2-456a-a599-08abcfec6d72",
   "metadata": {},
   "source": [
    "# Inference with pretrained Nucleotide Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de6889-1ceb-4acd-8765-1805a8b8bd6d",
   "metadata": {},
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549c2f1-bbdb-4fa7-847f-782a36cdca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be65e94-28db-4110-a147-7d8c05b44e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09aaf8-9738-422d-ade7-aaa10122fa67",
   "metadata": {},
   "source": [
    "## Download the weights\n",
    "The following cell allows you to download the weights of any of the four nucleotide transformer model. It returns the weights dictionary, the haiku forward function, the tokenizer and the config dictionary. Supported model names are **500M_human_ref**, **500M_1000G**, **2B5_1000G** and **2B5_multi_species**.\n",
    "\n",
    "Please also specify the layers at which you'd like to collect embeddings (e.g. (5, 10, 20) to get embeddings at layers 5, 10 and 20) and the maximum number of tokens in the sequences you'll compute the inference on. You can put values up to 1024 (counting the class token that will be added automatically at the beginning of the sequence), however we recommend keeping this number as small as possible for optimized memory and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3d8ab-ee0e-4592-b44f-a722e9372cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=\"500M_1000G\",\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    max_positions=32,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54878442-4c4b-4f65-b5fb-0a89aabb5ef5",
   "metadata": {},
   "source": [
    "## Define your input data and tokenize it\n",
    "You can have a look at the tokens_str variable to see how your sequences have been split into tokens. The sequences will all be padded to the value you filled for max_positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3dcba-2e12-4fa4-8e73-8494f97b6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and tokenize it\n",
    "sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9228f-ecc5-4ce0-99c2-c3062d6c19b4",
   "metadata": {},
   "source": [
    "## Do the Inference\n",
    "The first time you query this cell will be slower than usual inference because of the computation graph compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef8d85-31d8-4c72-97a9-c3ae4c1bdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize random key\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Infer\n",
    "outs = forward_fn.apply(parameters, random_key, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820ee47-7d03-47d9-b3f9-9faac32005d6",
   "metadata": {},
   "source": [
    "## Retrieve embeddings\n",
    "And use them as you please! Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756502bc-c1f9-44b7-b7dc-9b4c7806ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[\"embeddings_20\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf379-fa94-407f-babf-02d8fcbe9844",
   "metadata": {},
   "source": [
    "**Additional Tip**: Don't forget to remove the cls token and padded positions if you want for instance to compute mean embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2387a8-8edb-43d5-beaf-e129df0b2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outs[\"embeddings_20\"][:, 1:, :]  # removing CLS token\n",
    "padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "masked_embeddings = embeddings * padding_mask  # multiply by 0 pad tokens embeddings\n",
    "mean_embeddings = jnp.sum(masked_embeddings, axis=1) / jnp.sum(padding_mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a187a-265e-4125-b33c-3d2266d073c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33400aa3-c9ff-4af1-9818-e07f437fc00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
