{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a201d7bf-d4d2-456a-a599-08abcfec6d72",
   "metadata": {},
   "source": [
    "# Inference with pretrained Nucleotide Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f4cd74-0db3-4140-a2a6-e73a2c21c9d6",
   "metadata": {},
   "source": [
    "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/examples/inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de6889-1ceb-4acd-8765-1805a8b8bd6d",
   "metadata": {},
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28855f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549c2f1-bbdb-4fa7-847f-782a36cdca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install --no-deps git+https://github.com/instadeepai/nucleotide_transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be65e94-28db-4110-a147-7d8c05b44e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09aaf8-9738-422d-ade7-aaa10122fa67",
   "metadata": {},
   "source": [
    "## Download the weights\n",
    "The following cell allows you to download the weights of any of the four nucleotide transformer model. It returns the weights dictionary, the haiku forward function, the tokenizer and the config dictionary.\n",
    "\n",
    "Please also specify:\n",
    "1. the layers at which you'd like to collect embeddings (e.g. (5, 10, 20) to get embeddings at layers 5, 10 and 20)\n",
    "2. the attention maps youÂ´d like to collect (e.g. ((1,4), (7,18)) to get attention maps corresponding to layer 1 head number 4 and layer 7 head number 18). Please refer to the config to see the number of layers and heads in the model.\n",
    "3. the maximum number of tokens in the sequences you'll compute the inference on. You can put values up to value specified in the model's config (counting the class token that will be added automatically at the beginning of the sequence), however we recommend keeping this number as small as possible for optimized memory and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select a model\n",
    "#@markdown ---\n",
    "model_name = '500M_human_ref'#@param['500M_human_ref', '500M_1000G', '2B5_1000G', '2B5_multi_species']\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3d8ab-ee0e-4592-b44f-a722e9372cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    attention_maps_to_save=((1, 4), (7, 18)),\n",
    "    max_positions=32,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54878442-4c4b-4f65-b5fb-0a89aabb5ef5",
   "metadata": {},
   "source": [
    "## Define your input data and tokenize it\n",
    "You can have a look at the tokens_str variable to see how your sequences have been split into tokens. The sequences will all be padded to the value you filled for max_positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3dcba-2e12-4fa4-8e73-8494f97b6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and tokenize it\n",
    "sequences = [\n",
    "    \"ATTCCGAAATCGCTGACCGATCGTACGAAA\",\n",
    "    \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGATATCTCTCGAGCTAGC\",\n",
    "]\n",
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9228f-ecc5-4ce0-99c2-c3062d6c19b4",
   "metadata": {},
   "source": [
    "## Do the Inference\n",
    "The first time you query this cell will be slower than usual inference because of the computation graph compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef8d85-31d8-4c72-97a9-c3ae4c1bdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize random key\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Infer\n",
    "outs = forward_fn.apply(parameters, random_key, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b405a-823c-4208-bdb4-313cd21f64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820ee47-7d03-47d9-b3f9-9faac32005d6",
   "metadata": {},
   "source": [
    "## Retrieve embeddings\n",
    "And use them as you please! Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756502bc-c1f9-44b7-b7dc-9b4c7806ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs[\"embeddings_20\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf379-fa94-407f-babf-02d8fcbe9844",
   "metadata": {},
   "source": [
    "**Additional Tip**: Don't forget to remove the cls token and padded positions if you want for instance to compute mean embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2387a8-8edb-43d5-beaf-e129df0b2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outs[\"embeddings_20\"][:, 1:, :]  # removing CLS token\n",
    "padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "masked_embeddings = embeddings * padding_mask  # multiply by 0 pad tokens embeddings\n",
    "sequences_lengths = jnp.sum(padding_mask, axis=1)\n",
    "mean_embeddings = jnp.sum(masked_embeddings, axis=1) / sequences_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a187a-265e-4125-b33c-3d2266d073c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbc188-61b6-4bd8-81d9-4a6bc135e37c",
   "metadata": {},
   "source": [
    "## Get attention maps\n",
    "Here is an example on how to retrieve attention maps at a specific layer for a given head and how to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db064f47-fdb3-44d8-b259-c76041135c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs[\"attention_map_layer_1_number_4\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756c121-c203-46f7-a1a2-b85fd8f8f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# plot attention maps\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "seq_length0, seq_length1 = int(sequences_lengths[0]), int(sequences_lengths[1])\n",
    "\n",
    "# plot for first seq in the batch\n",
    "im0 = axes[0].imshow(\n",
    "    outs[\"attention_map_layer_1_number_4\"][\n",
    "        0, 1 : (seq_length0 + 1), 1 : (seq_length0 + 1)\n",
    "    ]\n",
    ")\n",
    "divider0 = make_axes_locatable(axes[0])\n",
    "cax0 = divider0.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "tokens0 = tokens_str[0][1 : (seq_length0 + 1)]\n",
    "axes[0].set_xticks(list(range(seq_length0)), tokens0, rotation=45)\n",
    "axes[0].set_yticks(list(range(seq_length0)), tokens0, rotation=45)\n",
    "fig.colorbar(im0, cax=cax0, orientation=\"vertical\")\n",
    "\n",
    "# plot for second seq in the batch\n",
    "im1 = axes[1].imshow(\n",
    "    outs[\"attention_map_layer_1_number_4\"][\n",
    "        1, 1 : (seq_length1 + 1), 1 : (seq_length1 + 1)\n",
    "    ]\n",
    ")\n",
    "divider1 = make_axes_locatable(axes[1])\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "tokens1 = tokens_str[1][1 : (seq_length1 + 1)]\n",
    "axes[1].set_xticks(list(range(seq_length1)), tokens1, rotation=45)\n",
    "axes[1].set_yticks(list(range(seq_length1)), tokens1, rotation=45)\n",
    "fig.colorbar(im1, cax=cax1, orientation=\"vertical\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fa9df-ae2e-4b87-aa6c-ab0ddce66fe5",
   "metadata": {},
   "source": [
    "## Get probabilities\n",
    "Finally, let's look at the model probabilities over the vocabulary at each position. These can be used notably to compute reconstruction accuracies and perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541310f1-0819-4c50-97f1-5fbb8afddd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outs[\"logits\"]\n",
    "probabilities = []\n",
    "\n",
    "# get probabilities separately for each seq as they have different lengths\n",
    "for seq_id in range(logits.shape[0]):\n",
    "\n",
    "    logits_seq = logits[seq_id]\n",
    "    seq_length = int(sequences_lengths[seq_id])\n",
    "    logits_seq = logits_seq[1 : (seq_length + 1)]  # remove CLS token and pads\n",
    "    probas = jax.nn.softmax(\n",
    "        logits_seq, axis=-1\n",
    "    )  # use softmax to transform logits into probabilities\n",
    "\n",
    "    print(probas.shape)\n",
    "    probabilities.append(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f8b51-54c2-47d0-8b00-9c5c4861db48",
   "metadata": {},
   "source": [
    "Let's look in particular at a given sequence and position and show the top-k probabilities and corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b6723-8a22-490d-9903-dee92898eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_id = 0\n",
    "position_id = 1\n",
    "\n",
    "probs = probabilities[sequence_id][position_id]\n",
    "sorted_positions = jnp.argsort(-probs)\n",
    "sorted_probs = probs[sorted_positions]\n",
    "\n",
    "top_k = 5\n",
    "for k in range(top_k):\n",
    "    predicted_token = tokenizer.id_to_token(int(sorted_positions[k]))\n",
    "    prob = sorted_probs[k]\n",
    "    print(f\"token: {predicted_token}, probability: {prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967f924-2867-4495-a495-6cf25522e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
