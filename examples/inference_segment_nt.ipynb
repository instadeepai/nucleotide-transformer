{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edrfY09jfn32"
   },
   "source": [
    "# Inference with Segment-NT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOC2A0oIfn36"
   },
   "source": [
    "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/examples/inference_segment_nt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWffCMcBfn37"
   },
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtaCigg-fn37",
    "outputId": "23bdb546-7a94-4590-bfd4-9cf868a787a7"
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install matplotlib\n",
    "!pip install biopython\n",
    "!pip install dm-haiku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alzkIxk9fn38"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mXG7gpvkasB",
    "outputId": "6a1551fb-4424-41fc-dd44-b7bcf4f78a59"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkTU4k4_fn39",
    "outputId": "106b3892-b75b-4314-a7e4-c2c72399c4fb"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from nucleotide_transformer.pretrained import get_pretrained_segment_nt_model\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "backend = \"gpu\"\n",
    "devices = jax.devices(backend)\n",
    "num_devices = len(devices)\n",
    "print(f\"Devices found: {devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hrGKA8ffn3-"
   },
   "source": [
    "## Download the weights\n",
    "The following cell allows you to download the weights of one of the Segment-NT models. It returns the weights dictionary, the haiku forward function, the tokenizer and the config dictionary.\n",
    "\n",
    "Just like for the `get_pretrained_nucleotide_transformer` function, you can also specify:\n",
    "1. the layers at which you'd like to collect embeddings (e.g. (5, 10, 20) to get embeddings at layers 5, 10 and 20)\n",
    "2. the attention maps youÂ´d like to collect (e.g. ((1,4), (7,18)) to get attention maps corresponding to layer 1 head number 4 and layer 7 head number 18). Please refer to the config to see the number of layers and heads in the model.\n",
    "3. the maximum number of tokens in the sequences you'll compute the inference on. You can put values up to value specified in the model's config (counting the class token that will be added automatically at the beginning of the sequence), however we recommend keeping this number as small as possible for optimized memory and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GPe-wKgfn3_",
    "outputId": "e14a1ae0-e57f-4034-df9d-d22396148b17"
   },
   "outputs": [],
   "source": [
    "# The number of DNA tokens (excluding the CLS token prepended) needs to be dividible by\n",
    "# the square of the number of downsampling block, i.e 4.\n",
    "max_num_nucleotides = 8332\n",
    "\n",
    "assert max_num_nucleotides % 4 == 0, (\n",
    "    \"The number of DNA tokens (excluding the CLS token prepended) needs to be dividible by\"\n",
    "     \"2 to the power of the number of downsampling block, i.e 4.\")\n",
    "\n",
    "# If max_num_nucleotides is larger than what was used to train Segment-NT, the rescaling\n",
    "# factor needs to be adapted.\n",
    "if max_num_nucleotides > 5001:\n",
    "    inference_rescaling_factor = max_num_nucleotides / 2048\n",
    "else:\n",
    "    inference_rescaling_factor=None\n",
    "\n",
    "# If this download fails at one point, restarting it will not work.\n",
    "# Before rerunning the cell, make sure to delete the cache by executing:\n",
    "# ! rm -rf ~/.cache/nucleotide_transformer/\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_segment_nt_model(\n",
    "    model_name=\"segment_nt\",\n",
    "    rescaling_factor=inference_rescaling_factor,\n",
    "    embeddings_layers_to_save=(29,),\n",
    "    attention_maps_to_save=((1, 4), (7, 10)),\n",
    "    max_positions=max_num_nucleotides + 1,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n",
    "apply_fn = jax.pmap(forward_fn.apply, devices=devices, donate_argnums=(0,))\n",
    "\n",
    "# Put all on devices\n",
    "random_key = jax.random.PRNGKey(seed=0)\n",
    "keys = jax.device_put_replicated(random_key, devices=devices)\n",
    "parameters = jax.device_put_replicated(parameters, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEZ5cze_Ayxl"
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9FgB3iiAyxm"
   },
   "source": [
    "To reproduce the Fig.3 of the Segment-NT paper, we retrieve here the file of the\n",
    "human chromosome 20 and select the corresponding 50kb sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atnql0qJAyxm"
   },
   "source": [
    "### Download fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX8gIHydfn4C",
    "outputId": "625b34ee-7f92-4863-f981-13ceab68b848"
   },
   "outputs": [],
   "source": [
    "! wget https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.20.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff5TzhdmAyxm"
   },
   "source": [
    "### Load sequence from chromosome and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq2gQncAfn4E"
   },
   "outputs": [],
   "source": [
    "fasta_path = \"Homo_sapiens.GRCh38.dna.chromosome.20.fa.gz\"\n",
    "\n",
    "with gzip.open(fasta_path, \"rt\") as handle:\n",
    "    record = next(SeqIO.parse(handle, \"fasta\"))\n",
    "    chr20 = str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AK_GVmKijpfL",
    "outputId": "d9efa96e-4235-4db5-acdf-4fdc3396f6d6"
   },
   "outputs": [],
   "source": [
    "idx_start = 5100000\n",
    "idx_stop = idx_start + max_num_nucleotides*6\n",
    "\n",
    "sequences = [chr20[idx_start:idx_stop]]\n",
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)[None, :]\n",
    "tokens.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7NLdUvAyxn"
   },
   "source": [
    "# Infer the corresponding probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luKpYA2Qjwqz",
    "outputId": "364c6171-b794-4898-c0a6-98d77dc17f84"
   },
   "outputs": [],
   "source": [
    "# Infer\n",
    "outs = apply_fn(parameters, keys, tokens)\n",
    "\n",
    "# Obtain the logits over the genomic features\n",
    "logits = outs[\"logits\"]\n",
    "# Transform them on probabilities\n",
    "probabilities = np.asarray(jax.nn.softmax(logits, axis=-1))[...,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE0JH1Zmfn4G"
   },
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPyM1lbUfn4G"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\n",
    "    \"notebook\",\n",
    "    font_scale=1,\n",
    "    rc={\n",
    "        \"font.size\": 14,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"axes.labelsize\": 18,\n",
    "        \"xtick.labelsize\": 16,\n",
    "        \"ytick.labelsize\": 16,\n",
    "        \"legend.fontsize\": 16,\n",
    "        }\n",
    ")\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "\n",
    "# set colors\n",
    "colors = sns.color_palette(\"Set2\").as_hex()\n",
    "colors2 = sns.color_palette(\"husl\").as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kEVjHNDfn4H"
   },
   "outputs": [],
   "source": [
    "def plot_features(\n",
    "    predicted_probabilities_all,\n",
    "    seq_length: int,\n",
    "    features: List[str],\n",
    "    order_to_plot: List[str],\n",
    "    fig_width=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to plot labels and predicted probabilities.\n",
    "\n",
    "    Args:\n",
    "        selected_features: selected feature names.\n",
    "        n_panels: number of panels, including top panel with labels.\n",
    "            For example, for two features per panel with 4 features, n_panels=3.\n",
    "        pdf: save PDF.\n",
    "        pdf_pages: pdf_pages object to add figures.\n",
    "    \"\"\"\n",
    "\n",
    "    sc = 1.8\n",
    "    n_panels = 7\n",
    "\n",
    "    # fig, axes = plt.subplots(n_panels, 1, figsize=(fig_width * sc, (n_panels + 2) * sc), height_ratios=[6] + [2] * (n_panels-1))\n",
    "    _, axes = plt.subplots(n_panels, 1, figsize=(fig_width * sc, (n_panels + 4) * sc))\n",
    "\n",
    "    for n, feat in enumerate(order_to_plot):\n",
    "        feat_id = features.index(feat)\n",
    "        prob_dist = predicted_probabilities_all[:, feat_id]\n",
    "\n",
    "        # Use the appropriate subplot\n",
    "        ax = axes[n // 2]\n",
    "\n",
    "        try:\n",
    "            id_color = colors[feat_id]\n",
    "        except:\n",
    "            id_color = colors2[feat_id - 8]\n",
    "        ax.plot(\n",
    "            prob_dist,\n",
    "            color=id_color,\n",
    "            label=feat,\n",
    "            linestyle=\"-\",\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "        ax.set_xlim(0, seq_length)\n",
    "        ax.grid(False)\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['top'].set_color('black')\n",
    "        ax.spines['right'].set_color('black')\n",
    "        ax.spines['left'].set_color('black')\n",
    "\n",
    "    for a in range (0,n_panels):\n",
    "        axes[a].set_ylim(0, 1.05)\n",
    "        axes[a].set_ylabel(\"Prob.\")\n",
    "        axes[a].legend(loc=\"upper left\", bbox_to_anchor=(1, 1), borderaxespad=0)\n",
    "        if a != (n_panels-1):\n",
    "            axes[a].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=False)\n",
    "\n",
    "    # Set common x-axis label\n",
    "    axes[-1].set_xlabel(\"Nucleotides\")\n",
    "    # axes[0].axis('off')  # Turn off the axis\n",
    "    axes[n_panels-1].grid(False)\n",
    "    axes[n_panels-1].tick_params(axis='y', which='both', left=True, right=False, labelleft=True, labelright=False)\n",
    "\n",
    "    axes[0].set_title(\"Probabilities predicted over all genomics features\", fontweight=\"bold\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcMUzrqvwCMm"
   },
   "outputs": [],
   "source": [
    "# Rearrange order of the features to match Fig.3 from the paper.\n",
    "features_rearranged = [\n",
    " 'protein_coding_gene',\n",
    " 'lncRNA',\n",
    " '5UTR',\n",
    " '3UTR',\n",
    " 'exon',\n",
    " 'intron',\n",
    " 'splice_donor',\n",
    " 'splice_acceptor',\n",
    " 'promoter_Tissue_specific',\n",
    " 'promoter_Tissue_invariant',\n",
    " 'enhancer_Tissue_specific',\n",
    " 'enhancer_Tissue_invariant',\n",
    " 'CTCF-bound',\n",
    " 'polyA_signal',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DqfGGpt0fn4I",
    "outputId": "95f0868b-227d-44b1-93f3-86e2c09a7d6e"
   },
   "outputs": [],
   "source": [
    "plot_features(\n",
    "    probabilities[0,0],\n",
    "    probabilities.shape[-2],\n",
    "    fig_width=20,\n",
    "    features=config.features,\n",
    "    order_to_plot=features_rearranged\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
