{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edrfY09jfn32"
      },
      "source": [
        "# Inference with ChatNT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOC2A0oIfn36"
      },
      "source": [
        "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/examples/inference_chatNT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWffCMcBfn37"
      },
      "source": [
        "## Installation and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtaCigg-fn37",
        "outputId": "555b9ef3-f72c-4957-a535-b9e4f53307c2"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n",
        "!pip install matplotlib\n",
        "!pip install biopython\n",
        "!pip install dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "alzkIxk9fn38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    import nucleotide_transformer\n",
        "except:\n",
        "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
        "    import nucleotide_transformer\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    from jax.tools import colab_tpu\n",
        "\n",
        "    colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkTU4k4_fn39",
        "outputId": "a04ca440-be95-49e1-b683-bf5b70d00777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Devices found: [CpuDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "from Bio import SeqIO\n",
        "import gzip\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from nucleotide_transformer.chatNT.pretrained import get_chatNT\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Specify your backend device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use either \"cpu\", \"gpu\" or \"tpu\"\n",
        "backend = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "devices = jax.devices(backend)\n",
        "num_devices = len(devices)\n",
        "print(f\"Devices found: {devices}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define function to generate answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(apply_fn, parameters, random_keys, english_tokenizer, english_tokens, bio_tokens, max_num_tokens_to_decode):\n",
        "    \"\"\"\n",
        "    Note: the function expects that pmap is already applied to the forward function, the inputs and the parameters\n",
        "    \"\"\"\n",
        "    english_tokens = english_tokens.copy()\n",
        "\n",
        "    idx_begin_generation = np.where(\n",
        "        english_tokens[0, 0] == english_tokenizer.pad_token_id\n",
        "    )[0][0]\n",
        "    projected_bio_embeddings = jax.device_put_replicated(None, devices=devices)\n",
        "    actual_nb_steps = 0\n",
        "\n",
        "    for _ in tqdm(range(max_num_tokens_to_decode)):\n",
        "        outs = apply_fn(\n",
        "            parameters,\n",
        "            random_keys,\n",
        "            multi_omics_tokens_ids=(english_tokens, bio_tokens),\n",
        "            projection_english_tokens_ids=english_tokens,\n",
        "            projected_bio_embeddings=projected_bio_embeddings,\n",
        "        )\n",
        "        projected_bio_embeddings = outs[\"projected_bio_embeddings\"]\n",
        "        logits = outs[\"logits\"]\n",
        "\n",
        "        first_idx_pad_token = np.where(\n",
        "            english_tokens[0, 0] == english_tokenizer.pad_token_id\n",
        "        )[0][0]\n",
        "        predicted_token = np.argmax(logits[0, 0, first_idx_pad_token - 1])\n",
        "\n",
        "        if predicted_token == english_tokenizer.eos_token_id:\n",
        "            break\n",
        "        else:\n",
        "            english_tokens = english_tokens.at[0, 0, first_idx_pad_token].set(\n",
        "                predicted_token\n",
        "            )\n",
        "            actual_nb_steps += 1\n",
        "\n",
        "    decoded_generated_sentence = english_tokenizer.decode(\n",
        "        english_tokens[0, 0, idx_begin_generation : idx_begin_generation + actual_nb_steps]\n",
        "    )\n",
        "\n",
        "    return decoded_generated_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model's weights...\n"
          ]
        }
      ],
      "source": [
        "forward_fn, parameters, english_tokenizer, bio_tokenizer = get_chatNT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "forward_fn = hk.transform(forward_fn)\n",
        "apply_fn = jax.pmap(forward_fn.apply, devices=devices, donate_argnums=(0,))\n",
        "\n",
        "# Put required quantities for the inference on the devices. This step is not\n",
        "# reproduced in the second inference since the quantities will already be loaded\n",
        "# on the devices !\n",
        "random_key = jax.random.PRNGKey(seed=0)\n",
        "random_keys = jax.numpy.stack([random_key for _ in range(len(devices))])\n",
        "keys = jax.device_put_replicated(random_key, devices=devices)\n",
        "parameters = jax.device_put_replicated(parameters, devices=devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define custom inputs (note that the number of <DNA> token in the english sequence must be equal to len(dna_sequences))\n",
        "english_sequence = \"A chat between a curious user and an artificial intelligence assistant that can handle bio sequences. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Is there any evidence of an acceptor splice site in this sequence <DNA> ? ASSISTANT:\"\n",
        "dna_sequences = [\"ATCGGAAAAAGATCCAGAAAGTTATACCAGGCCAATGGGAATCACCTATTACGTGGATAATAGCGATAGTATGTTACCTATAAATTTAACTACGTGGATATCAGGCAGTTACGTTACCAGTCAAGGAGCACCCAAAACTGTCCAGCAACAAGTTAATTTACCCATGAAGATGTACTGCAAGCCTTGCCAACCAGTTAAAGTAGCTACTCATAAGGTAATAAACAGTAATATCGACTTTTTATCCATTTTGATAATTGATTTATAACAGTCTATAACTGATCGCTCTACATAATCTCTATCAGATTACTATTGACACAAACAGAAACCCCGTTAATTTGTATGATATATTTCCCGGTAAGCTTCGATTTTTAATCCTATCGTGACAATTTGGAATGTAACTTATTTCGTATAGGATAAACTAATTTACACGTTTGAATTCCTAGAATATGGAGAATCTAAAGGTCCTGGCAATGCCATCGGCTTTCAATATTATAATGGACCAAAAGTTACTCTATTAGCTTCCAAAACTTCGCGTGAGTACATTAGAACAGAAGAATAACCTTCAATATCGAGAGAGTTACTATCACTAACTATCCTATG\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "english_max_length = 512 # length of the tokenized english sequence\n",
        "bio_tokenized_sequence_length = 512 # length of the tokenized DNA sequences\n",
        "\n",
        "english_tokens = english_tokenizer(\n",
        "    [english_sequence],\n",
        "    return_tensors=\"np\",\n",
        "    max_length=english_max_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ").input_ids\n",
        "\n",
        "bio_tokens = bio_tokenizer(\n",
        "    dna_sequences,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\",\n",
        "    max_length=bio_tokenized_sequence_length,\n",
        "    truncation=True,\n",
        ").input_ids\n",
        "bio_tokens = np.expand_dims(bio_tokens, axis=0) # Add batch dimension -> result: (1, num_dna_sequences, bio_tokenized_sequence_length)\n",
        "\n",
        "\n",
        "# Replicate over devices\n",
        "english_tokens = jnp.stack([jnp.asarray(english_tokens, dtype=jnp.int32)]*num_devices, axis=0)\n",
        "bio_tokens = jnp.stack([jnp.asarray(bio_tokens, dtype=jnp.int32)]*num_devices, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOOWXYluU7p"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_answer = generate_answer(\n",
        "    apply_fn=apply_fn,\n",
        "    parameters=parameters,\n",
        "    random_keys=random_keys,\n",
        "    english_tokenizer=english_tokenizer,\n",
        "    english_tokens=english_tokens,\n",
        "    bio_tokens=bio_tokens,\n",
        "    max_num_tokens_to_decode=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, an acceptor splice site is present in this nucleotide sequence.\n"
          ]
        }
      ],
      "source": [
        "print(generated_answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genomics-research-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
