{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edrfY09jfn32"
   },
   "source": [
    "# Inference with sCellTransformer - Jax version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/notebooks/sct/inference_sCT_jax_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWffCMcBfn37"
   },
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alzkIxk9fn38",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    !pip install anndata\n",
    "    !pip install cellxgene_census\n",
    "    !pip install scanpy\n",
    "    !pip install jax\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:42.565213Z",
     "start_time": "2025-06-06T08:05:39.457648Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkTU4k4_fn39",
    "outputId": "a04ca440-be95-49e1-b683-bf5b70d00777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices found: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import os\n",
    "import json\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from scipy.sparse import issparse\n",
    "from typing import Any\n",
    "import cellxgene_census\n",
    "\n",
    "from nucleotide_transformer.sCellTransformer.model import build_sct_fn\n",
    "from nucleotide_transformer.sCellTransformer.params import download_ckpt\n",
    "\n",
    "# Specify \"cpu\" as default (but you can decide to use GPU or TPU in the next cell)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify your backend device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use either \"cpu\", \"gpu\" or \"tpu\"\n",
    "backend = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = jax.devices(backend)\n",
    "num_devices = len(devices)\n",
    "print(f\"Devices found: {devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:43.842658Z",
     "start_time": "2025-06-06T08:05:42.566420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model's weights...\n"
     ]
    }
   ],
   "source": [
    "parameters, config = download_ckpt()\n",
    "forward_fn = build_sct_fn(config, name=\"long_range_nt\")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:43.852716Z",
     "start_time": "2025-06-06T08:05:43.845707Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Create simple input (no splitting)\n",
    "dummy_batch_size = 1\n",
    "dummy_sequence_length = 19968 * 50 \n",
    "dummy_tokens = np.zeros((dummy_batch_size, dummy_sequence_length), dtype=np.int32)\n",
    "dummy_tokens = jax.device_put_replicated(dummy_tokens, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:48.049844Z",
     "start_time": "2025-06-06T08:05:47.937338Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Setup devices and keys\n",
    "devices = jax.local_devices()\n",
    "num_devices = len(devices)\n",
    "master_key = jax.random.PRNGKey(seed=0)\n",
    "keys_batch = jax.random.split(master_key, num_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:48.771060Z",
     "start_time": "2025-06-06T08:05:48.767384Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Create pmap function (input not split, just replicated)\n",
    "apply_fn = jax.pmap(\n",
    "    forward_fn.apply,\n",
    "    in_axes=(None, 0, 0),  # params: replicated, keys: split, data: replicated\n",
    "    devices=devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:06:48.616283Z",
     "start_time": "2025-06-06T08:05:49.434331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running forward pass...\n",
      "✓ SUCCESS!\n",
      "Output keys:\n",
      "  conv_out: (1, 1, 3900, 1024)\n",
      "  deconv_out: (1, 1, 64, 998400)\n",
      "  embedding: (1, 1, 50, 19968, 64)\n",
      "  logits: (1, 1, 998400, 7)\n",
      "  transformer_out: (1, 1, 3900, 1024)\n",
      "Logits shape: (1, 1, 998400, 7)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 5. Run the forward pass\n",
    "print(\"Running forward pass...\")\n",
    "try:\n",
    "    outs = apply_fn(parameters, keys_batch, dummy_tokens)\n",
    "    print(\"✓ SUCCESS!\")\n",
    "    \n",
    "    if isinstance(outs, dict):\n",
    "        print(\"Output keys:\")\n",
    "        for key, value in outs.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"  {key}: {value.shape}\")\n",
    "        \n",
    "        if \"logits\" in outs:\n",
    "            logits = outs[\"logits\"]\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            predictions = np.asarray(np.argmax(logits[0,:,:,:5], axis=-1))\n",
    "            print(\"Done!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate example from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the h5ad file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:07:55.967158Z",
     "start_time": "2025-06-06T08:06:48.618867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 42.8M/42.8M [00:53<00:00, 842kB/s] \n"
     ]
    }
   ],
   "source": [
    "# Downloading the file from the public API of cellxgene\n",
    "# This file is a h5ad file containing single-cell RNA-seq data\n",
    "# It corresponds to Sst Chodl - MTG: Seattle Alzheimer's Disease Atlas (SEA-AD)\n",
    "# - Single-cell RNA-seq data: cells x genes expression matrix\n",
    "# - Sparse data (~90% zeros), typically 16k cells, ~30k genes\n",
    "# - Contains cell type annotations (neurons, astrocytes, etc.) and metadata\n",
    "# - Real biological data vs synthetic test data - will need preprocessing for model\n",
    "# Open the Census (using the same version as your S3 path: 2023-12-15)\n",
    "cellxgene_census.download_source_h5ad(\n",
    "    \"81e91ff8-f619-4ad1-a0c3-b45e1dc63f68\",\n",
    "    to_path=\"brain.h5ad\",\n",
    "    census_version=\"2023-12-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:08:32.599835Z",
     "start_time": "2025-06-06T08:08:32.566016Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loading mapping from ENSEMBL name to index in the dataset.\n",
    "with open(os.path.join(current_dir, \"data/ensembl_id_vocab.json\"), \"r\") as f:\n",
    "    ENSEMBL_ID_VOCAB = json.load(f)\n",
    "\n",
    "# Loading mapping, for the considered coding genes,\n",
    "# between global index in the dataset and their index among coding genes only.\n",
    "# Restricting from 60k genes to 20k genes only. \n",
    "with open(os.path.join(current_dir, \"data/protein_gene_map.json\"), \"r\") as f:\n",
    "    PROTEIN_GENE_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:08:34.481126Z",
     "start_time": "2025-06-06T08:08:33.806067Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_mapping_between_adata_and_model(adata: ad.AnnData,\n",
    "                                           ENSEMBL_ID_VOCAB: dict,\n",
    "                                           PROTEIN_GENE_MAP: dict) -> np.ndarray:\n",
    "    # Define mapping\n",
    "    names = list(adata.var.feature_name.keys())\n",
    "    MAP_TO_PROTEIN_GENE_INDEX = {}\n",
    "    indexes_present_in_data = {}\n",
    "    for i, name in enumerate(names):\n",
    "        if name in ENSEMBL_ID_VOCAB:\n",
    "            index = str(ENSEMBL_ID_VOCAB[name])\n",
    "            if index in PROTEIN_GENE_MAP:\n",
    "                indexes_present_in_data[index] = 1\n",
    "                MAP_TO_PROTEIN_GENE_INDEX[str(i)] = PROTEIN_GENE_MAP[index]\n",
    "\n",
    "    # Create gene mapping arrays\n",
    "    gene_map = {int(k): MAP_TO_PROTEIN_GENE_INDEX[k] for k in MAP_TO_PROTEIN_GENE_INDEX}\n",
    "    new_gene_map_array = np.full(70000, -1, dtype=np.int32)\n",
    "    for k, v in gene_map.items():\n",
    "        new_gene_map_array[k] = v\n",
    "    return new_gene_map_array\n",
    "\n",
    "\n",
    "# Note that this data download already includes a log normalization \n",
    "# on the gene expression levels.\n",
    "adata = sc.read_h5ad('brain.h5ad')\n",
    "# Creating the mapping between indexes in the downloaded dataset and \n",
    "# the index in the model for the considered coding genes. \n",
    "new_gene_map_array = define_mapping_between_adata_and_model(\n",
    "    adata=adata,\n",
    "    ENSEMBL_ID_VOCAB=ENSEMBL_ID_VOCAB,\n",
    "    PROTEIN_GENE_MAP=PROTEIN_GENE_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:08:34.491975Z",
     "start_time": "2025-06-06T08:08:34.482235Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_h5ad_scrna_dataset(\n",
    "        adata: Any,\n",
    "        new_gene_map_array: np.ndarray,\n",
    "        num_downsamples: int,\n",
    "        cell_len: int,\n",
    "        num_cells: int,\n",
    "        pad_token_id: int,\n",
    "        gene_expression_num_bins: int,\n",
    "        batch_size: int,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Creates an iterable dataset from h5ad file for single-cell RNA-seq data.\n",
    "    \n",
    "    Args:\n",
    "        h5ad_path: Path to the h5ad file\n",
    "        new_gene_map_array: Array mapping new gene indices to your previous index system\n",
    "        num_downsamples: Number of downsampling steps\n",
    "        cell_len: Length of each cell in the dataset\n",
    "        num_cells: Number of cells per sample\n",
    "        pad_token_id: Token ID for padding\n",
    "        gene_expression_num_bins: Number of bins for gene expression\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        An iterable dataset that yields batches of samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract expression matrix (usually X is sparse)\n",
    "    expr_matrix = adata.X\n",
    "    if issparse(expr_matrix):\n",
    "        # Convert to CSR for efficient row access\n",
    "        expr_matrix = expr_matrix.tocsr()\n",
    "\n",
    "    # Calculate sequence length with downsampling\n",
    "    downsample_factor = 2 ** num_downsamples\n",
    "    seq_length = math.ceil(cell_len / downsample_factor) * downsample_factor\n",
    "\n",
    "    class H5adIterableDataset:\n",
    "        def __init__(self):\n",
    "            self.length = cell_len\n",
    "            self.batch_size = batch_size\n",
    "            self.total_cells = expr_matrix.shape[0]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.length\n",
    "\n",
    "        def __iter__(self):\n",
    "            # Create infinite iterator over cell indices\n",
    "            cell_indices = itertools.cycle(range(self.total_cells))\n",
    "\n",
    "            while True:\n",
    "                batch = []\n",
    "\n",
    "                # Generate batch_size samples\n",
    "                for _ in range(self.batch_size):\n",
    "                    cells = []\n",
    "\n",
    "                    # Collect num_cells cells for one sample\n",
    "                    while len(cells) < num_cells:\n",
    "                        cell_idx = next(cell_indices)\n",
    "\n",
    "                        # Get expression data for this cell\n",
    "                        if issparse(expr_matrix):\n",
    "                            # For sparse matrix, get the row as a dense array\n",
    "                            cell_expr = expr_matrix[cell_idx, :].toarray().flatten()\n",
    "                        else:\n",
    "                            cell_expr = expr_matrix[cell_idx, :]\n",
    "\n",
    "                        # Find non-zero expressions\n",
    "                        non_zero_mask = cell_expr > 0\n",
    "                        gene_idxs = np.where(non_zero_mask)[0].astype(np.int32)\n",
    "                        expressions = cell_expr[non_zero_mask].astype(np.float32)\n",
    "\n",
    "                        if len(gene_idxs) == 0:\n",
    "                            # Skip cells with no expression\n",
    "                            continue\n",
    "\n",
    "                        # Map genes using new_gene_map_array\n",
    "                        mapped_idxs = new_gene_map_array[gene_idxs]\n",
    "                        valid_mask = mapped_idxs != -1\n",
    "                        positions = mapped_idxs[valid_mask]\n",
    "                        valid_expr = expressions[valid_mask]\n",
    "\n",
    "                        if len(valid_expr) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Bin expressions\n",
    "                        if min(valid_expr) == max(valid_expr):\n",
    "                            bin_edges = np.array(\n",
    "                                [min(valid_expr) - 0.1, max(valid_expr) + 0.1])\n",
    "                            binned = np.ones_like(valid_expr, dtype=np.int32)\n",
    "                        else:\n",
    "                            bin_edges = np.linspace(\n",
    "                                min(valid_expr),\n",
    "                                max(valid_expr),\n",
    "                                gene_expression_num_bins,\n",
    "                            )\n",
    "                            bin_edges[-1] += 0.01\n",
    "                            binned = np.digitize(valid_expr, bin_edges)\n",
    "\n",
    "                        # Create full arrays (using the original gene_ids from your code)\n",
    "                        full_expr = np.zeros(self.length, dtype=np.int32)\n",
    "                        full_expr[positions] = binned\n",
    "\n",
    "                        raw_expr = np.zeros(self.length, dtype=np.float32)\n",
    "                        raw_expr[positions] = valid_expr\n",
    "\n",
    "                        # Create cell dictionary\n",
    "                        cell = {\n",
    "                            # \"gene_ids\": gene_ids.copy(),\n",
    "                            \"gene_expressions\": full_expr,\n",
    "                            \"raw_gene_expressions\": raw_expr,\n",
    "                            \"bins\": bin_edges,\n",
    "                            \"source\": [\"h5ad\"],\n",
    "                        }\n",
    "\n",
    "                        # Pad if needed\n",
    "                        if seq_length > self.length:\n",
    "                            for key in [\"gene_expressions\",\n",
    "                                        \"raw_gene_expressions\"]:\n",
    "                                pad_value = pad_token_id\n",
    "                                padded = np.full(seq_length, pad_value,\n",
    "                                                 dtype=cell[key].dtype)\n",
    "                                padded[:len(cell[key])] = cell[key]\n",
    "                                cell[key] = padded\n",
    "\n",
    "                        cells.append(cell)\n",
    "\n",
    "                    # Merge cells for this sample\n",
    "                    if len(cells) == num_cells:\n",
    "                        sample = {}\n",
    "\n",
    "                        # Merge arrays by concatenation\n",
    "                        for key in cells[0]:\n",
    "                            if isinstance(cells[0][key], np.ndarray):\n",
    "                                sample[key] = np.concatenate([c[key] for c in cells])\n",
    "                            else:\n",
    "                                sample[key] = list(itertools.chain.from_iterable(\n",
    "                                    c[key] for c in cells\n",
    "                                ))\n",
    "\n",
    "                        batch.append(sample)\n",
    "\n",
    "                # Reshape and yield batch\n",
    "                if len(batch) == self.batch_size:\n",
    "                    batch_result = {}\n",
    "\n",
    "                    for key in batch[0]:\n",
    "                        if isinstance(batch[0][key], np.ndarray):\n",
    "                            batch_arrays = [b[key] for b in batch]\n",
    "                            batch_result[key] = np.stack(batch_arrays, axis=0)\n",
    "                        else:\n",
    "                            batch_result[key] = [b[key] for b in batch]\n",
    "\n",
    "                    yield batch_result\n",
    "\n",
    "    return H5adIterableDataset()\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataloader = get_h5ad_scrna_dataset(\n",
    "    adata=adata,\n",
    "    new_gene_map_array=new_gene_map_array,\n",
    "    num_downsamples=config.num_downsamples,\n",
    "    cell_len=len(PROTEIN_GENE_MAP),\n",
    "    num_cells=config.num_cells,\n",
    "    pad_token_id=config.pad_token_id,\n",
    "    gene_expression_num_bins=5,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:08:35.672716Z",
     "start_time": "2025-06-06T08:08:35.663803Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_gene_expression_imputation(\n",
    "        train_dataloader: DataLoader,\n",
    "        num_batches: int|None = 10,\n",
    "        mask_ratio: float = 0.15,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a gene expression imputation model using Matthews Correlation Coefficient.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader: DataLoader containing gene expression data\n",
    "        num_batches: Number of batches to evaluate. If None, all batches are evaluated\n",
    "        mask_ratio: Ratio of tokens to mask for imputation\n",
    "\n",
    "    Returns:\n",
    "        float: Average Matthews Correlation Coefficient across all batches\n",
    "    \"\"\"\n",
    "    # Lists to store true and predicted values for masked tokens\n",
    "    all_true_values = []\n",
    "    all_pred_values = []\n",
    "\n",
    "    # Iterate over specified number of batches\n",
    "    iterator = iter(train_dataloader)\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches)):\n",
    "        try:\n",
    "            # Get next batch\n",
    "            batch = next(iterator)\n",
    "\n",
    "            # Move batch to the same device as model\n",
    "            gene_expressions = jnp.array(batch[\"gene_expressions\"])\n",
    "\n",
    "            # Create random mask\n",
    "            mask = jax.random.uniform(jax.random.PRNGKey(0), shape=gene_expressions.shape) < mask_ratio\n",
    "\n",
    "            # Clone and mask gene expressions\n",
    "            masked_gene_expressions = jnp.where(mask, config.mask_token_id, gene_expressions)\n",
    "    \n",
    "            # Keep original values before masking for evaluation\n",
    "            true_values = np.asarray(gene_expressions[mask])\n",
    "    \n",
    "            # Convert to jax and replicate over devices\n",
    "            masked_gene_expressions = jnp.array(masked_gene_expressions)\n",
    "            masked_gene_expressions = jnp.expand_dims(masked_gene_expressions, axis=0)\n",
    "            random_key = jax.random.PRNGKey(seed=0)\n",
    "            keys = jax.device_put_replicated(random_key, devices=devices)\n",
    "    \n",
    "            # Forward pass without gradient computation\n",
    "            outs = apply_fn(parameters, keys, masked_gene_expressions) \n",
    "            logits = outs[\"logits\"]\n",
    "    \n",
    "            # Get predictions (assuming classification - adjust if regression)\n",
    "            predictions = np.asarray(np.argmax(logits[0,:,:,:5], axis=-1))\n",
    "            pred_values = predictions[mask]\n",
    "            \n",
    "            # Store true and predicted values\n",
    "            all_true_values.append(true_values)\n",
    "            all_pred_values.append(pred_values)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(f\"DataLoader exhausted after {batch_idx} batches\")\n",
    "            break\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_true_values = np.concatenate(all_true_values)\n",
    "    all_pred_values = np.concatenate(all_pred_values)\n",
    "\n",
    "    # Compute Matthews Correlation Coefficient\n",
    "    # - Binary classification metric ranging from -1 to +1\n",
    "    # - +1: perfect prediction, 0: random prediction, -1: total disagreement\n",
    "    # - Balanced metric that works well with imbalanced datasets\n",
    "    # - Considers all confusion matrix elements (TP, TN, FP, FN)\n",
    "\n",
    "    mcc = matthews_corrcoef(all_true_values, all_pred_values)\n",
    "    print(f\"Overall MCC: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:09:39.390864Z",
     "start_time": "2025-06-06T08:08:43.981415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:55<00:00, 55.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MCC: 0.3703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Please define the number of batches and the mask ratio for evaluation\n",
    "# The reported metric is the average MCC across all batches computed over the bins.\n",
    "evaluate_gene_expression_imputation(\n",
    "    dataloader,\n",
    "    num_batches=1,\n",
    "    mask_ratio=0.15,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "debug_segment_enformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
