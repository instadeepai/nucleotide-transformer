{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edrfY09jfn32"
   },
   "source": [
    "# Inference with sCellTransformer - Jax version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/nucleotide-transformer/blob/main/notebooks/sct/inference_sCT_jax_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWffCMcBfn37"
   },
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:09:26.826264Z",
     "start_time": "2025-06-05T18:09:26.816031Z"
    },
    "id": "alzkIxk9fn38"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:09:39.371606Z",
     "start_time": "2025-06-05T18:09:27.425033Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkTU4k4_fn39",
    "outputId": "a04ca440-be95-49e1-b683-bf5b70d00777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices found: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import os\n",
    "import json\n",
    "import anndata as ad\n",
    "import boto3\n",
    "import botocore\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from scipy.sparse import issparse\n",
    "from typing import Any\n",
    "\n",
    "from nucleotide_transformer.sCellTransformer.model import build_sct_fn\n",
    "from nucleotide_transformer.sCellTransformer.params import download_ckpt\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "backend = \"cpu\"\n",
    "devices = jax.devices(backend)\n",
    "num_devices = len(devices)\n",
    "print(f\"Devices found: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:09:40.523421Z",
     "start_time": "2025-06-05T18:09:39.376498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model's weights...\n"
     ]
    }
   ],
   "source": [
    "parameters, config = download_ckpt()\n",
    "forward_fn = build_sct_fn(config, name=\"long_range_nt\")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:14:15.108054Z",
     "start_time": "2025-06-05T18:14:15.103163Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Create simple input (no splitting)\n",
    "dummy_batch_size = 1\n",
    "dummy_sequence_length = 19968 * 50 \n",
    "dummy_tokens = np.zeros((dummy_batch_size, dummy_sequence_length), dtype=np.int32)\n",
    "dummy_tokens = jax.device_put_replicated(dummy_tokens, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:14:20.262836Z",
     "start_time": "2025-06-05T18:14:20.258051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 998400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:09:40.808883Z",
     "start_time": "2025-06-05T18:09:40.545098Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Setup devices and keys\n",
    "devices = jax.local_devices()\n",
    "num_devices = len(devices)\n",
    "master_key = jax.random.PRNGKey(seed=0)\n",
    "keys_batch = jax.random.split(master_key, num_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:09:40.813211Z",
     "start_time": "2025-06-05T18:09:40.810359Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Create pmap function (input not split, just replicated)\n",
    "apply_fn = jax.pmap(\n",
    "    forward_fn.apply,\n",
    "    in_axes=(None, 0, 0),  # params: replicated, keys: split, data: replicated\n",
    "    devices=devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:15:29.741307Z",
     "start_time": "2025-06-05T18:14:23.255794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running forward pass...\n",
      "✓ SUCCESS!\n",
      "Output keys:\n",
      "  conv_out: (1, 1, 3900, 1024)\n",
      "  deconv_out: (1, 1, 64, 998400)\n",
      "  embedding: (1, 1, 50, 19968, 64)\n",
      "  logits: (1, 1, 998400, 7)\n",
      "  transformer_out: (1, 1, 3900, 1024)\n",
      "Logits shape: (1, 1, 998400, 7)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 5. Run the forward pass\n",
    "print(\"Running forward pass...\")\n",
    "try:\n",
    "    outs = apply_fn(parameters, keys_batch, dummy_tokens)\n",
    "    print(\"✓ SUCCESS!\")\n",
    "    \n",
    "    if isinstance(outs, dict):\n",
    "        print(\"Output keys:\")\n",
    "        for key, value in outs.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"  {key}: {value.shape}\")\n",
    "        \n",
    "        if \"logits\" in outs:\n",
    "            logits = outs[\"logits\"]\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            predictions = np.asarray(np.argmax(logits[0,:,:,:5], axis=-1))\n",
    "            print(\"Done!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate example from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the h5ad file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:22:07.501901Z",
     "start_time": "2025-06-05T18:21:53.347553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize S3 client with no signing configuration\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    config=botocore.config.Config(signature_version=botocore.UNSIGNED)\n",
    ")\n",
    "\n",
    "# Downloading the file from the public API of cellxgene\n",
    "# This file is a h5ad file containing single-cell RNA-seq data\n",
    "# It corresponds to Sst Chodl - MTG: Seattle Alzheimer's Disease Atlas (SEA-AD)\n",
    "# - Single-cell RNA-seq data: cells x genes expression matrix\n",
    "# - Sparse data (~90% zeros), typically 16k cells, ~30k genes\n",
    "# - Contains cell type annotations (neurons, astrocytes, etc.) and metadata\n",
    "# - Real biological data vs synthetic test data - will need preprocessing for model\n",
    "s3.download_file(\n",
    "    'cellxgene-data-public',\n",
    "    'cell-census/2023-12-15/h5ads/81e91ff8-f619-4ad1-a0c3-b45e1dc63f68.h5ad',\n",
    "    'brain.h5ad'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:22:07.532583Z",
     "start_time": "2025-06-05T18:22:07.503370Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loading mapping from ENSEMBL name to index in the dataset.\n",
    "with open(os.path.join(current_dir, \"data/ensembl_id_vocab.json\"), \"r\") as f:\n",
    "    ENSEMBL_ID_VOCAB = json.load(f)\n",
    "\n",
    "# Loading mapping, for the considered coding genes,\n",
    "# between global index in the dataset and their index among coding genes only.\n",
    "# Restricting from 60k genes to 20k genes only. \n",
    "with open(os.path.join(current_dir, \"data/protein_gene_map.json\"), \"r\") as f:\n",
    "    PROTEIN_GENE_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:22:08.278844Z",
     "start_time": "2025-06-05T18:22:07.534403Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_mapping_between_adata_and_model(adata: ad.AnnData,\n",
    "                                           ENSEMBL_ID_VOCAB: dict,\n",
    "                                           PROTEIN_GENE_MAP: dict) -> np.ndarray:\n",
    "    # Define mapping\n",
    "    names = list(adata.var.feature_name.keys())\n",
    "    MAP_TO_PROTEIN_GENE_INDEX = {}\n",
    "    indexes_present_in_data = {}\n",
    "    for i, name in enumerate(names):\n",
    "        if name in ENSEMBL_ID_VOCAB:\n",
    "            index = str(ENSEMBL_ID_VOCAB[name])\n",
    "            if index in PROTEIN_GENE_MAP:\n",
    "                indexes_present_in_data[index] = 1\n",
    "                MAP_TO_PROTEIN_GENE_INDEX[str(i)] = PROTEIN_GENE_MAP[index]\n",
    "\n",
    "    # Create gene mapping arrays\n",
    "    gene_map = {int(k): MAP_TO_PROTEIN_GENE_INDEX[k] for k in MAP_TO_PROTEIN_GENE_INDEX}\n",
    "    new_gene_map_array = np.full(70000, -1, dtype=np.int32)\n",
    "    for k, v in gene_map.items():\n",
    "        new_gene_map_array[k] = v\n",
    "    return new_gene_map_array\n",
    "\n",
    "\n",
    "# Note that this data download already includes a log normalization \n",
    "# on the gene expression levels.\n",
    "adata = sc.read_h5ad('brain.h5ad')\n",
    "# Creating the mapping between indexes in the downloaded dataset and \n",
    "# the index in the model for the considered coding genes. \n",
    "new_gene_map_array = define_mapping_between_adata_and_model(\n",
    "    adata=adata,\n",
    "    ENSEMBL_ID_VOCAB=ENSEMBL_ID_VOCAB,\n",
    "    PROTEIN_GENE_MAP=PROTEIN_GENE_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:22:08.291478Z",
     "start_time": "2025-06-05T18:22:08.280797Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_h5ad_scrna_dataset(\n",
    "        adata: Any,\n",
    "        new_gene_map_array: np.ndarray,\n",
    "        num_downsamples: int,\n",
    "        cell_len: int,\n",
    "        num_cells: int,\n",
    "        pad_token_id: int,\n",
    "        gene_expression_num_bins: int,\n",
    "        batch_size: int,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Creates an iterable dataset from h5ad file for single-cell RNA-seq data.\n",
    "    \n",
    "    Args:\n",
    "        h5ad_path: Path to the h5ad file\n",
    "        new_gene_map_array: Array mapping new gene indices to your previous index system\n",
    "        num_downsamples: Number of downsampling steps\n",
    "        cell_len: Length of each cell in the dataset\n",
    "        num_cells: Number of cells per sample\n",
    "        pad_token_id: Token ID for padding\n",
    "        gene_expression_num_bins: Number of bins for gene expression\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        An iterable dataset that yields batches of samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract expression matrix (usually X is sparse)\n",
    "    expr_matrix = adata.X\n",
    "    if issparse(expr_matrix):\n",
    "        # Convert to CSR for efficient row access\n",
    "        expr_matrix = expr_matrix.tocsr()\n",
    "\n",
    "    # Calculate sequence length with downsampling\n",
    "    downsample_factor = 2 ** num_downsamples\n",
    "    seq_length = math.ceil(cell_len / downsample_factor) * downsample_factor\n",
    "\n",
    "    class H5adIterableDataset:\n",
    "        def __init__(self):\n",
    "            self.length = cell_len\n",
    "            self.batch_size = batch_size\n",
    "            self.total_cells = expr_matrix.shape[0]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.length\n",
    "\n",
    "        def __iter__(self):\n",
    "            # Create infinite iterator over cell indices\n",
    "            cell_indices = itertools.cycle(range(self.total_cells))\n",
    "\n",
    "            while True:\n",
    "                batch = []\n",
    "\n",
    "                # Generate batch_size samples\n",
    "                for _ in range(self.batch_size):\n",
    "                    cells = []\n",
    "\n",
    "                    # Collect num_cells cells for one sample\n",
    "                    while len(cells) < num_cells:\n",
    "                        cell_idx = next(cell_indices)\n",
    "\n",
    "                        # Get expression data for this cell\n",
    "                        if issparse(expr_matrix):\n",
    "                            # For sparse matrix, get the row as a dense array\n",
    "                            cell_expr = expr_matrix[cell_idx, :].toarray().flatten()\n",
    "                        else:\n",
    "                            cell_expr = expr_matrix[cell_idx, :]\n",
    "\n",
    "                        # Find non-zero expressions\n",
    "                        non_zero_mask = cell_expr > 0\n",
    "                        gene_idxs = np.where(non_zero_mask)[0].astype(np.int32)\n",
    "                        expressions = cell_expr[non_zero_mask].astype(np.float32)\n",
    "\n",
    "                        if len(gene_idxs) == 0:\n",
    "                            # Skip cells with no expression\n",
    "                            continue\n",
    "\n",
    "                        # Map genes using new_gene_map_array\n",
    "                        mapped_idxs = new_gene_map_array[gene_idxs]\n",
    "                        valid_mask = mapped_idxs != -1\n",
    "                        positions = mapped_idxs[valid_mask]\n",
    "                        valid_expr = expressions[valid_mask]\n",
    "\n",
    "                        if len(valid_expr) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Bin expressions\n",
    "                        if min(valid_expr) == max(valid_expr):\n",
    "                            bin_edges = np.array(\n",
    "                                [min(valid_expr) - 0.1, max(valid_expr) + 0.1])\n",
    "                            binned = np.ones_like(valid_expr, dtype=np.int32)\n",
    "                        else:\n",
    "                            bin_edges = np.linspace(\n",
    "                                min(valid_expr),\n",
    "                                max(valid_expr),\n",
    "                                gene_expression_num_bins,\n",
    "                            )\n",
    "                            bin_edges[-1] += 0.01\n",
    "                            binned = np.digitize(valid_expr, bin_edges)\n",
    "\n",
    "                        # Create full arrays (using the original gene_ids from your code)\n",
    "                        full_expr = np.zeros(self.length, dtype=np.int32)\n",
    "                        full_expr[positions] = binned\n",
    "\n",
    "                        raw_expr = np.zeros(self.length, dtype=np.float32)\n",
    "                        raw_expr[positions] = valid_expr\n",
    "\n",
    "                        # Create cell dictionary\n",
    "                        cell = {\n",
    "                            # \"gene_ids\": gene_ids.copy(),\n",
    "                            \"gene_expressions\": full_expr,\n",
    "                            \"raw_gene_expressions\": raw_expr,\n",
    "                            \"bins\": bin_edges,\n",
    "                            \"source\": [\"h5ad\"],\n",
    "                        }\n",
    "\n",
    "                        # Pad if needed\n",
    "                        if seq_length > self.length:\n",
    "                            for key in [\"gene_expressions\",\n",
    "                                        \"raw_gene_expressions\"]:\n",
    "                                pad_value = pad_token_id\n",
    "                                padded = np.full(seq_length, pad_value,\n",
    "                                                 dtype=cell[key].dtype)\n",
    "                                padded[:len(cell[key])] = cell[key]\n",
    "                                cell[key] = padded\n",
    "\n",
    "                        cells.append(cell)\n",
    "\n",
    "                    # Merge cells for this sample\n",
    "                    if len(cells) == num_cells:\n",
    "                        sample = {}\n",
    "\n",
    "                        # Merge arrays by concatenation\n",
    "                        for key in cells[0]:\n",
    "                            if isinstance(cells[0][key], np.ndarray):\n",
    "                                sample[key] = np.concatenate([c[key] for c in cells])\n",
    "                            else:\n",
    "                                sample[key] = list(itertools.chain.from_iterable(\n",
    "                                    c[key] for c in cells\n",
    "                                ))\n",
    "\n",
    "                        batch.append(sample)\n",
    "\n",
    "                # Reshape and yield batch\n",
    "                if len(batch) == self.batch_size:\n",
    "                    batch_result = {}\n",
    "\n",
    "                    for key in batch[0]:\n",
    "                        if isinstance(batch[0][key], np.ndarray):\n",
    "                            batch_arrays = [b[key] for b in batch]\n",
    "                            batch_result[key] = np.stack(batch_arrays, axis=0)\n",
    "                        else:\n",
    "                            batch_result[key] = [b[key] for b in batch]\n",
    "\n",
    "                    yield batch_result\n",
    "\n",
    "    return H5adIterableDataset()\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataloader = get_h5ad_scrna_dataset(\n",
    "    adata=adata,\n",
    "    new_gene_map_array=new_gene_map_array,\n",
    "    num_downsamples=config.num_downsamples,\n",
    "    cell_len=len(PROTEIN_GENE_MAP),\n",
    "    num_cells=config.num_cells,\n",
    "    pad_token_id=config.pad_token_id,\n",
    "    gene_expression_num_bins=5,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:22:14.914901Z",
     "start_time": "2025-06-05T18:22:14.907146Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_gene_expression_imputation(\n",
    "        train_dataloader: DataLoader,\n",
    "        num_batches: int|None = 10,\n",
    "        mask_ratio: float = 0.15,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a gene expression imputation model using Matthews Correlation Coefficient.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader: DataLoader containing gene expression data\n",
    "        num_batches: Number of batches to evaluate. If None, all batches are evaluated\n",
    "        mask_ratio: Ratio of tokens to mask for imputation\n",
    "\n",
    "    Returns:\n",
    "        float: Average Matthews Correlation Coefficient across all batches\n",
    "    \"\"\"\n",
    "    # Lists to store true and predicted values for masked tokens\n",
    "    all_true_values = []\n",
    "    all_pred_values = []\n",
    "\n",
    "    # Iterate over specified number of batches\n",
    "    iterator = iter(train_dataloader)\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches)):\n",
    "        try:\n",
    "            # Get next batch\n",
    "            batch = next(iterator)\n",
    "\n",
    "            # Move batch to the same device as model\n",
    "            gene_expressions = jnp.array(batch[\"gene_expressions\"])\n",
    "\n",
    "            # Create random mask\n",
    "            mask = jax.random.uniform(jax.random.PRNGKey(0), shape=gene_expressions.shape) < mask_ratio\n",
    "\n",
    "            # Clone and mask gene expressions\n",
    "            masked_gene_expressions = jnp.where(mask, config.mask_token_id, gene_expressions)\n",
    "    \n",
    "            # Keep original values before masking for evaluation\n",
    "            true_values = np.asarray(gene_expressions[mask])\n",
    "    \n",
    "            # Convert to jax and replicate over devices\n",
    "            masked_gene_expressions = jnp.array(masked_gene_expressions)\n",
    "            masked_gene_expressions = jnp.expand_dims(masked_gene_expressions, axis=0)\n",
    "            random_key = jax.random.PRNGKey(seed=0)\n",
    "            keys = jax.device_put_replicated(random_key, devices=devices)\n",
    "    \n",
    "            # Forward pass without gradient computation\n",
    "            outs = apply_fn(parameters, keys, masked_gene_expressions) \n",
    "            logits = outs[\"logits\"]\n",
    "    \n",
    "            # Get predictions (assuming classification - adjust if regression)\n",
    "            predictions = np.asarray(np.argmax(logits[0,:,:,:5], axis=-1))\n",
    "            pred_values = predictions[mask]\n",
    "            \n",
    "            # Store true and predicted values\n",
    "            all_true_values.append(true_values)\n",
    "            all_pred_values.append(pred_values)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(f\"DataLoader exhausted after {batch_idx} batches\")\n",
    "            break\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_true_values = np.concatenate(all_true_values)\n",
    "    all_pred_values = np.concatenate(all_pred_values)\n",
    "\n",
    "    # Compute Matthews Correlation Coefficient\n",
    "    # - Binary classification metric ranging from -1 to +1\n",
    "    # - +1: perfect prediction, 0: random prediction, -1: total disagreement\n",
    "    # - Balanced metric that works well with imbalanced datasets\n",
    "    # - Considers all confusion matrix elements (TP, TN, FP, FN)\n",
    "\n",
    "    mcc = matthews_corrcoef(all_true_values, all_pred_values)\n",
    "    print(f\"Overall MCC: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:23:23.794119Z",
     "start_time": "2025-06-05T18:22:16.022794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:07<00:00, 67.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 998400)\n",
      "Overall MCC: 0.3703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Please define the number of batches and the mask ratio for evaluation\n",
    "# The reported metric is the average MCC across all batches computed over the bins.\n",
    "evaluate_gene_expression_imputation(\n",
    "    dataloader,\n",
    "    num_batches=1,\n",
    "    mask_ratio=0.15,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "genomics_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
