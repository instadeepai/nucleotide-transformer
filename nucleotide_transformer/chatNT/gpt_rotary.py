from dataclasses import dataclass
from typing import Optional, Tuple

import haiku as hk
import jax.numpy as jnp
import numpy as np


def create_sinusoidal_positions(num_pos: int, dim: int, theta: float) -> np.ndarray:
    """
    Create the sinus and cosines for the RoPE

    Args:
        num_pos: the number of position to encode
        dim: the dimension of the RoPE
        theta: rotation angle

    Returns:
        Array of size (num_pos, 2*dim) containing the sinus and cosinus for RoPE
    """

    inv_freq = 1.0 / (theta ** (np.arange(0, dim, 2) / dim))
    sinusoid_inp = np.einsum("i , j -> i j", np.arange(num_pos), inv_freq)
    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)

    sentinel = dim // 2 + dim % 2
    jmp_policy = hk.mixed_precision.current_policy()
    if jmp_policy is None:
        # default float32
        compute_dtype = np.float32
    else:
        # cast to jmp policy if specified
        compute_dtype = jmp_policy.compute_dtype

    sincos = np.zeros((num_pos, dim), dtype=compute_dtype)
    sincos[:, 0:sentinel] = sin
    sincos[:, sentinel:] = cos

    return np.array(sincos)


def rotate_every_two(attention_tensor: jnp.ndarray) -> jnp.ndarray:
    """
    Prepare a tensor to apply the RoPE mechanism

    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor

    Returns:
        The even indices in the last dimension have their sign flipped
        tensor size : (batch_size, seq_len, num_heads, key_dim)
    """
    rotate_half_tensor = jnp.stack(
        (-attention_tensor[:, :, :, 1::2], attention_tensor[:, :, :, ::2]), axis=-1
    )
    rotate_half_tensor = rotate_half_tensor.reshape(
        rotate_half_tensor.shape[:-2] + (-1,)
    )
    return rotate_half_tensor


def apply_rotary_pos_emb(
    attention_tensor: jnp.ndarray, sincos: jnp.ndarray
) -> jnp.ndarray:
    """
    Apply the RoPE to attention_tensor
    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor
        sincos: the sincos generated by the function 'create_sinusoidal_positions'
            shape :

    Returns:
        the corresponding RoPE-encoded tensor
    """
    sin_pos, cos_pos = sincos
    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
    return (attention_tensor * cos_pos) + (rotate_every_two(attention_tensor) * sin_pos)


@dataclass
class RotaryEmbeddingConfig:
    """
    Rotary Positional Embedding configuration
        max_seq_len: The number of positions to encode and cache.
        dim: Dimension of RoPE.
        theta: Rotation angle.
    """

    max_seq_len: int
    dim: int
    theta: float


class RotaryEmbedding(hk.Module):
    """
    Rotary Positional Embedding inspired by GPT-like models (LLAMA and GPTJ).
    """

    def __init__(
        self,
        config: RotaryEmbeddingConfig,
        name: Optional[str] = None,
    ):
        """
        Args:
            config: Rotary Positional Embedding configuration.
            name: Name of the layer. Defaults to None.
        """
        super().__init__(name=name)
        self.max_seq_len = config.max_seq_len
        self.dim = config.dim
        self.theta = config.theta
        self.sincos_cache = self._create_sinusoidal_positions()

    def _create_sinusoidal_positions(self) -> np.ndarray:
        """
        Create the sines and cosines for the RoPE.

        Returns:
            Sinusoidal positions of shape (self.max_seq_len, self.dim).
        """
        inv_freq = 1.0 / (self.theta ** (np.arange(0, self.dim, 2) / self.dim))
        sinusoid_inp = np.einsum("i , j -> i j", np.arange(self.max_seq_len), inv_freq)
        sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)
        sentinel = self.dim // 2 + self.dim % 2
        jmp_policy = hk.mixed_precision.current_policy()
        if jmp_policy is None:
            compute_dtype = np.float32
        else:
            compute_dtype = jmp_policy.compute_dtype
        sincos = np.zeros((self.max_seq_len, self.dim), dtype=compute_dtype)
        sincos[:, 0:sentinel] = sin
        sincos[:, sentinel:] = cos
        return np.array(sincos)

    def _rotate_every_two(self, x: jnp.ndarray) -> jnp.ndarray:
        """
        Prepare a tensor to apply the RoPE mechanism.

        Args:
            x: Tensor of shape (batch_size, seq_len, num_heads, head_dim),
               typically this is the key or query tensor.

        Returns:
            The even indices in the last dimension have their sign flipped.
            Tensor of shape (batch_size, seq_len, num_heads, head_dim).
        """
        rotate_half = jnp.stack((-x[:, :, :, 1::2], x[:, :, :, ::2]), axis=-1)
        rotate_half = rotate_half.reshape(rotate_half.shape[:-2] + (-1,))
        return rotate_half

    def _apply_rotary_pos_emb(self, x: jnp.ndarray, sincos: jnp.ndarray) -> jnp.ndarray:
        """
        Applies rotary embeddings to x.

        Args:
            x: Tensor of shape (batch_size, seq_len, num_heads, head_dim),
               typically this is the key or query tensor.
        Returns:
            Rope embeddings tensor.
        """
        sin_pos, cos_pos = sincos
        sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
        cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
        return (x * cos_pos) + (self._rotate_every_two(x) * sin_pos)

    def __call__(
        self, k: jnp.ndarray, q: jnp.ndarray, positions: Optional[jnp.ndarray] = None
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Applies rotary embeddings to k and q.

        Args:
            k: key tensor of shape (batch_size, seq_len, num_heads, head_dim),
            q: value tensor of shape (batch_size, seq_len, num_heads, head_dim),
            positions: optional positions offset useful when caching,

        Returns:
            RoPE embeddings for the keys and values.
        """
        position_ids = jnp.arange(0, k.shape[1], 1, dtype=jnp.int32)
        position_ids = jnp.expand_dims(position_ids, 0).repeat(k.shape[0], 0)
        if positions is not None:
            position_ids += positions
        sincos = jnp.take(self.sincos_cache, position_ids, axis=0)
        sincos = jnp.split(sincos, 2, axis=-1)
        k_rot = self._apply_rotary_pos_emb(k[:, :, :, : self.dim], sincos)
        k_pass = k[:, :, :, self.dim :]
        q_rot = self._apply_rotary_pos_emb(q[:, :, :, : self.dim], sincos)
        q_pass = q[:, :, :, self.dim :]
        keys = jnp.concatenate([k_rot, k_pass], axis=-1)
        values = jnp.concatenate([q_rot, q_pass], axis=-1)
        return keys, values
